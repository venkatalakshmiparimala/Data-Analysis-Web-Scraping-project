ğŸ“˜ Data Analysis & Web Scraping 
This repository contains two assignments that demonstrate skills in data engineering, data extraction, and web scraping using Python and R. Both projects reflect a practical application of real-world data processing and dynamic web content extraction.

ğŸ“Œ Assignment 1: JSON Flattening and Data Enrichment (DMCA Notices)
ğŸ“‚ Objective
â€¢	Process a deeply nested JSON file containing DMCA takedown notices.

â€¢	Flatten and enrich the data.

â€¢	Generate analytical summaries using both Python and R.

ğŸ§  Approach
1. Pre-analysis
â€¢	Used JSONLint to understand and validate the structure.

â€¢	Reorganized the JSON into a more readable format for inspection.

2. Flattening the JSON
â€¢	Iterated through notices, works, and infringing URLs.

â€¢	Extracted key metadata: notice ID, sender, principal name, work description.

Output: flattened_step1.csv.

3. Adding Domain and IP
Extracted domain from each infringing URL.

Resolved domain to IP using:

â€¢	Python: socket.gethostbyname() with ThreadPoolExecutor.

â€¢	R: nslookup() with parSapply() on a 4-core cluster.

Output: flattened_step3.csv.

4. Summarizations
â€¢	Top 10 infringing domains.

â€¢	Unique notices per sender.

â€¢	Infringing URLs grouped by work description.

â€¢	Additional metrics: most active senders/domains.

ğŸ› ï¸ Tech Stack
â€¢	Languages: Python, R

â€¢	Python Libraries: pandas, json, urllib.parse, socket, concurrent.futures

â€¢	R Packages: jsonlite, dplyr, urltools, parallel

âœ… Outcome
Successfully built a scalable and parallelized data pipeline.

Generated CSV summaries with clean code in both languages.

ğŸ“Œ Assignment 2: Web Scraping Journal Articles (SAGE)
ğŸ“‚ Objective
Scrape article metadata from the Journal of Marketing (SAGE) current issue page, including:

â€¢	Title

â€¢	Authors

â€¢	First Published Date

â€¢	DOI

â€¢	Abstract

ğŸ§  Approach
âŒ Attempt 1: requests + BeautifulSoup
Could not retrieve article data due to JavaScript-rendered content.

âœ… Final Solution: Selenium + BeautifulSoup
â€¢	Controlled Chrome via Selenium to simulate browser behavior.

â€¢	Accepted cookies and waited for JS execution.

â€¢	Extracted and parsed article content using BeautifulSoup.

Saved data into a structured CSV.

ğŸ› ï¸ Tech Stack
â€¢	Python 3.x

â€¢	Selenium + ChromeDriver

â€¢	BeautifulSoup (bs4)

â€¢	pandas

â€¢	requests (explored, not used in final solution)

âœ… Outcome
â€¢	Extracted full article metadata.

â€¢	Saved output as assignment2_articles_final.csv.

â€¢	Selenium proved reliable for dynamic web scraping.

ğŸ“ Deliverables
âœ… Clean and well-documented Python & R scripts

âœ… Output CSVs for both assignments

âœ… Summary documents outlining approach and results
